## Copyright (c) 2024, 2025, Oracle and/or its affiliates.
## Licensed under the Universal Permissive License v1.0 as shown at http://oss.oracle.com/licenses/upl.
# spell-checker: ignore mxbai

# replicaCount: number of desired pod replicas for the Deployment
replicaCount: 1

# imagePullSecrets: Secret name containing image pull secrets
imagePullSecrets: []
  # - name: registry_cred

# image: image and tag to pull incl policy
image:
  repository: docker.io/ollama/ollama
  tag: "latest"
imagePullPolicy: IfNotPresent

# service: type of Kubernetes service to expose the HTTP port.
service:
  http:
    type: "ClusterIP"

# models: to automatically deploy if enabled
models:
  enabled: true
  modelPullList:
    - llama3.1
    - mxbai-embed-large

# livenessProbe: defines settings for the liveness probe.
# Set `enabled` to false to disable the probe entirely.
livenessProbe:
  enabled: true
  initialDelaySeconds: 20       # Time (in seconds) to wait before starting the probe
  periodSeconds: 20             # Time (in seconds) between probe checks
  timeoutSeconds: 1             # Time (in seconds) to wait for probe response
  failureThreshold: 3          # Number of failures before marking the container as unhealthy
  successThreshold: 1           # Number of successes required to consider the container healthy

# readinessProbe: defines settings for the liveness probe.
# Set `enabled` to false to disable the probe entirely.
readinessProbe:
  enabled: true
  initialDelaySeconds: 10
  periodSeconds: 15
  timeoutSeconds: 1
  failureThreshold: 3
  successThreshold: 1 

# podLabels: additional key:value labels to add to pods
podLabels: {}
  # environment: "production"
  # team: "devops"

# podAnnotations: additional key:value annotations to add to the Pod template metadata
podAnnotations: {}
  # prometheus.io/scrape: "true"
  # prometheus.io/port: "9102"

# resources: requests and limits for the container
# Often used to ensure pod is running on a GPU worker
resources: {}
  # limits:
  #   nvidia.com/gpu: 1

# nodeSelector: to constrain pods to specific nodes
nodeSelector: {}
  # disktype: ssd

# affinity: rules for scheduling pods
affinity: {}
  # podAntiAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     - labelSelector:
  #         matchExpressions:
  #           - key: app
  #             operator: In
  #             values:
  #               - my-app
  #       topologyKey: "kubernetes.io/hostname"

# tolerations: for scheduling pods on tainted nodes
tolerations: []
  # - key: "key1"
  #   operator: "Exists"
  #   effect: "NoSchedule"